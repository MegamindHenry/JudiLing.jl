"""
    Pyndl_Weight_Struct
        cues::Vector{String}
        outcomes::Vector{String}
        weight::Matrix{Float64}

- `cues::Vector{String}`: Vector of cues, in the order that they appear in the weight matrix.
- `outcomes::Vector{String}`: Vector of outcomes, in the order that they appear in the weight matrix.
- `weight::Matrix{Float64}`: Weight matrix.
"""
struct Pyndl_Weight_Struct
    cues::Vector{String}
    outcomes::Vector{String}
    weight::Matrix{Float64}
end

"""
    pyndl(
        data_path::String;
        alpha::Float64 = 0.1,
        betas::Tuple{Float64,Float64} = (0.1, 0.1),
        method::String = "openmp"
    )

Compute weights using pyndl. See the documentation of pyndl for more information: https://pyndl.readthedocs.io/en/latest/

# Obligatory arguments
- `data_path::String`: Path to an events file as generated by pyndl's preprocess.create_event_file

# Optional arguments
- `alpha::Float64 = 0.1`: α learning rate.
- `betas::Tuple{Float64,Float64} = (0.1, 0.1)`: β_1 and β_2 learning rates
- `method::String = "openmp"`: One of {"openmp", "threading"}. "openmp" only works on Linux.

# Example
```julia
weights = JudiLing.pyndl("data/latin_train_events.tab.gz")
```
"""
function pyndl(
    data_path::String;
    alpha::Float64 = 0.1,
    betas::Tuple{Float64,Float64} = (0.1, 0.1),
    method::String = "openmp"
)

    ndl = pyimport("pyndl.ndl")

    weights_py = ndl.ndl(
        events = data_path,
        alpha = alpha,
        betas = betas,
        method = method,
        remove_duplicates = true,
    )

    unwrap_xarray(weights_py)

end

function unwrap_xarray(weights)
    coords = weights.coords.to_dataset()
    cues = [i for i in coords.cues.data]
    outcomes = [i for i in coords.outcomes.data]
    weights = weights.data

    Pyndl_Weight_Struct(cues, outcomes, weights)
end


"""
    make_cue_matrix(
        data::DataFrame,
        pyndl_weights::Pyndl_Weight_Struct;
        grams = 3,
        target_col = "Words",
        tokenized = false,
        sep_token = nothing,
        keep_sep = false,
        start_end_token = "#",
        verbose = false,
    )

Make the cue matrix based on a dataframe and weights computed with pyndl. Practically this means that the cues are extracted from the weights object and translated to the JudiLing format.

# Obligatory arguments
- `data::DataFrame`: Dataset with all the word types on which the weights were trained.
- `pyndl_weights::Pyndl_Weight_Struct`: Weights trained with JudiLing.pyndl

# Optional argyments
- `grams = 3`: N-gram size (has to match the n-gram granularity of the cues on which the weights were trained).
- `target_col = "Words"`: Column with target words.
- `tokenized = false`: Whether the target words are already tokenized
- `sep_token = nothing`: The string separating the tokens (only used if `tokenized=true`).
- `keep_sep = false`: Whether the `sep_token` should be retained in the cues.
- `start_end_token = "#"`: The string with which to mark word boundaries.
- `verbose = false`: Verbose mode.

# Example
```julia
weights = JudiLing.pyndl("data/latin_train_events.tab.gz")
cue_obj = JudiLing.make_cue_matrix("latin_train.csv", weights,
                                    grams = 3,
                                    target_col = "Word")
```
"""
function make_cue_matrix(
    data::DataFrame,
    pyndl_weights::Pyndl_Weight_Struct;
    grams = 3,
    target_col = "Words",
    tokenized = false,
    sep_token = nothing,
    keep_sep = false,
    start_end_token = "#",
    verbose = false,
)

    # split tokens from words or other columns
    if tokenized && !isnothing(sep_token)
        tokens = split.(data[:, target_col], sep_token)
    else
        tokens = split.(data[:, target_col], "")
    end

    # making ngrams from tokens
    # make_ngrams function are below
    ngrams = make_ngrams.(tokens, grams, keep_sep, sep_token, start_end_token)

    # find all unique ngrams features
    ngrams_features = unique(vcat(ngrams...))

    f2i = Dict(v => i for (i, v) in enumerate(pyndl_weights.cues))
    i2f = Dict(i => v for (i, v) in enumerate(pyndl_weights.cues))

    n_f = sum([length(v) for v in ngrams])

    m = size(data, 1)
    n = length(pyndl_weights.cues)
    I = zeros(Int64, n_f)
    J = zeros(Int64, n_f)
    V = ones(Int64, n_f)

    A = [Int64[] for i = 1:length(pyndl_weights.cues)]

    cnt = 0
    for (i, v) in enumerate(ngrams)
        last = 0
        for (j, f) in enumerate(v)
            cnt += 1
            I[cnt] = i
            fi = f2i[f]
            J[cnt] = fi
            if j == 1
                last = fi
            else
                push!(A[last], fi)
                last = fi
            end
        end
    end

    cue = sparse(I, J, V, m, n, *)

    ngrams_ind = [[f2i[x] for x in y] for y in ngrams]

    verbose && println("making adjacency matrix...")
    A = [sort(unique(i)) for i in A]
    n_adj = sum(length.(A))
    I = zeros(Int64, n_adj)
    J = zeros(Int64, n_adj)
    V = ones(Int64, n_adj)

    cnt = 0
    iter = enumerate(A)
    if verbose
        pb = Progress(length(A))
    end
    for (i, v) in iter
        for j in v
            cnt += 1
            I[cnt] = i
            J[cnt] = j
        end
        if verbose
            ProgressMeter.next!(pb)
        end
    end

    A = sparse(I, J, V, length(f2i), length(f2i))

    Cue_Matrix_Struct(cue, f2i, i2f, ngrams_ind, A, grams, target_col,
        tokenized, sep_token, keep_sep, start_end_token)
end


"""
    make_S_matrix(
        data_train::DataFrame,
        data_val::DataFrame,
        pyndl_weights::Pyndl_Weight_Struct,
        n_features_columns::Vector;
        tokenized::Bool=false,
        sep_token::String="_"
    )

Create semantic matrix based on a training and validation dataframe and weights computed with pyndl. Practically this means that the semantic features are extracted from the weights object and translated to the JudiLing format.

# Obligatory arguments
- `data_train::DataFrame`: The training dataset.
- `data_val::DataFrame`: The validation dataset.
- `pyndl_weights::Pyndl_Weight_Struct`: Weights trained with JudiLing.pyndl.
- `n_features_columns::Vector`: Vector of columns with the features in the training and validation datasets.

# Optional arguments
- `tokenized=false`: Whether the features in `n_features_columns` columns are already tokenized (e.g. `"feature1_feature2_feature3"`)
- `sep_token="_"`: The string with which the features are separated (only used if `tokenized=false`).

# Example
```julia
weights = JudiLing.pyndl("data/latin_train_events.tab.gz")
S_train, S_val = JudiLing.make_S_matrix(train,
                            val,
                            weights_latin,
                            ["Lexeme", "Person", "Number", "Tense", "Voice", "Mood"],
                            tokenized=false)
```
"""
function make_S_matrix(
    data_train::DataFrame,
    data_val::DataFrame,
    pyndl_weights::Pyndl_Weight_Struct,
    n_features_columns::Vector;
    tokenized::Bool=false,
    sep_token::String="_"
)

    f2i = Dict(v => i for (i, v) in enumerate(pyndl_weights.outcomes))
    i2f = Dict(i => v for (i, v) in enumerate(pyndl_weights.outcomes))

    n_f = length(pyndl_weights.outcomes)

    St_train = zeros(Float64, n_f, size(data_train, 1))
    for i = 1:size(data_train, 1)
        for f in data_train[i, n_features_columns]
            if tokenized
                for f_i in split(f, sep_token)
                    St_train[f2i[f_i], i] = 1
                end
            else
                St_train[f2i[f], i] = 1
            end
        end
    end

    St_val = zeros(Float64, n_f, size(data_val, 1))
    for i = 1:size(data_val, 1)
        for f in data_val[i, n_features_columns]
            if tokenized
                for f_i in split(f, sep_token)
                    St_val[f2i[f_i], i] = 1
                end
            else
                St_val[f2i[f], i] = 1
            end
        end
    end

    St_train', St_val'
end


"""
    make_S_matrix(
        data::DataFrame,
        pyndl_weights::Pyndl_Weight_Struct,
        n_features_columns::Vector;
        tokenized::Bool=false,
        sep_token::String="_"
    )

Create semantic matrix based on a dataframe and weights computed with pyndl. Practically this means that the semantic features are extracted from the weights object and translated to the JudiLing format.

# Obligatory arguments
- `data::DataFrame`: The dataset with word types.
- `pyndl_weights::Pyndl_Weight_Struct`: Weights trained with JudiLing.pyndl.
- `n_features_columns::Vector`: Vector of columns with the features in the dataset.

# Optional arguments
- `tokenized=false`: Whether the features in `n_features_columns` columns are already tokenized (e.g. `"feature1_feature2_feature3"`)
- `sep_token="_"`: The string with which the features are separated (only used if `tokenized=false`).

# Example
```julia
weights = JudiLing.pyndl("data/latin_train_events.tab.gz")
S = JudiLing.make_S_matrix(data,
                            weights_latin,
                            ["Lexeme", "Person", "Number", "Tense", "Voice", "Mood"],
                            tokenized=false)
```
"""
function make_S_matrix(
    data::DataFrame,
    pyndl_weights::Pyndl_Weight_Struct,
    n_features_columns::Vector;
    tokenized::Bool=false,
    sep_token::String="_"
)

    f2i = Dict(v => i for (i, v) in enumerate(pyndl_weights.outcomes))
    i2f = Dict(i => v for (i, v) in enumerate(pyndl_weights.outcomes))

    n_f = length(pyndl_weights.outcomes)

    St = zeros(Float64, n_f, size(data, 1))
    for i = 1:size(data, 1)
        for f in data[i, n_features_columns]
            if tokenized
                for f_i in split(f, sep_token)
                    St[f2i[f_i], i] = 1
                end
            else
                St[f2i[f], i] = 1
            end
        end
    end

    St'
end
